{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750c2aec-adce-494c-b598-b6d47158ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in d:\\python\\python313\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in d:\\python\\python313\\lib\\site-packages (from h5py) (2.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in d:\\python\\python313\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in d:\\python\\python313\\lib\\site-packages (from h5py) (2.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n",
    "!pip3 install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "912faae8-8d01-4218-8cab-5f6580e9e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py # common package to interact with a dataset that is stored on an H5 file.\n",
    "import scipy\n",
    "# from PIL import Image\n",
    "from scipy import ndimage\n",
    "from scipy.stats import logistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775bfb56-894c-4693-86a5-787d741754bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\VAmazinumCamp2025\\\\jupyter\\\\lesson_7_test_folder\\\\data'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "path = os.path.join(cwd,'data') \n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8b939f-f7ed-4d3d-8d90-9b4661accc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    file_name=  os.path.join(path , 'train_catvnoncat.h5')\n",
    "    train_dataset = h5py.File(file_name, \"r\")\n",
    "    X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    \n",
    "    file_name=  os.path.join(path , 'test_catvnoncat.h5')\n",
    "    test_dataset = h5py.File(file_name, \"r\")\n",
    "    X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = ['non-cat','cat']\n",
    "    \n",
    "    Y_train = Y_train.reshape(-1,1)\n",
    "    Y_test = Y_test.reshape(-1,1)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb552fa-93a6-4770-9ac2-002307a80ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train, X_test, Y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d214d6f4-2d20-4a45-8437-2163a7e925c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=  (209, 64, 64, 3)\n",
      "X_test.shape=  (50, 64, 64, 3)\n",
      "Y_train.shape=  (209, 1)\n",
      "Y_test.shape=  (50, 1)\n"
     ]
    }
   ],
   "source": [
    "print ('X_train.shape= ',X_train.shape)\n",
    "print ('X_test.shape= ',X_test.shape)\n",
    "print ('Y_train.shape= ',Y_train.shape)\n",
    "print ('Y_test.shape= ',Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e155e2-3348-4269-8da4-87534268af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = X_train.shape[0]\n",
    "num_px = 64\n",
    "m_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a1dd6d5-754d-4ffc-bf99-c9ed2c5c7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc41717f-63c5-4205-a5ab-9db215f9baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = X_train.reshape(m_train, -1)\n",
    "X_test_flatten =  X_test.reshape(m_test, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a705996a-58ce-4fe1-9fe7-60571b9f4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (209, 12288)\n",
      "test_set_x_flatten shape: (50, 12288)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x_flatten shape: {}\".format(X_train_flatten.shape))\n",
    "print (\"test_set_x_flatten shape: {}\".format(X_test_flatten.shape))\n",
    "print (\"sanity check after reshaping: {}\".format(X_train_flatten[0, :5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40e64a5-d11c-49c7-9b87-c87e1f7d10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_flatten/255.\n",
    "X_test_scaled = X_test_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0976f209-6536-46e8-bf1e-5781a91726bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    g -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    g = logistic.cdf(z)  \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "184e1137-239c-4cde-a159-2cbcd0d552b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25caa417-fc57-4c1b-9f40-db81b9445f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (1,dim) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (1,dim)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR_CODE.  Initialize b to zero and w as a vector of zeros. \n",
    "    # START_CODE   \n",
    "    w = np.zeros((1, dim))\n",
    "    b = 0\n",
    "    # END_CODE \n",
    "\n",
    "    assert(w.shape == (1,dim))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8dac60af-fa58-4006-890d-a6e5f42ae731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0. 0.]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3323e53d-0cbc-41f3-9b2c-ef4906b359b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y, C=1):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (number of examples,1)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    " \n",
    "    # YOUR_CODE.  implement forward propagation \n",
    "    # START_CODE   \n",
    "    Z= b + X @ w.T\n",
    "    print(X.shape)\n",
    "    print(w.T)\n",
    "    print(Z.shape)\n",
    "    #Z = np.dot(X, w.T) + b\n",
    "    A=  1 / (1 + np.exp(-Z))\n",
    "    #A = logistic.cdf(Z)  \n",
    "    cost = -np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m\n",
    "    \n",
    "    # END_CODE \n",
    "    \n",
    "    # YOUR_CODE.  Implement Backward propahation \n",
    "    # START_CODE   \n",
    "    dJ_dw = (A - Y).T @ X / m\n",
    "    #dJ_dw = np.dot((A - Y).T, X) / m\n",
    "    dJ_db = np.sum(A - Y) / m\n",
    "\n",
    "   # END_CODE \n",
    "\n",
    "    assert(dJ_dw.shape == w.shape)\n",
    "    assert(dJ_db.dtype == float)\n",
    "    assert(cost.dtype == float)\n",
    "    \n",
    "    grads = {\"dJ_dw\": dJ_dw,\n",
    "             \"dJ_db\": dJ_db}\n",
    "    #print(f\"Z: {Z}\")\n",
    "    #print(f\"A: {A}\")\n",
    "    #print(f\"Y: {Y}\")\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5f76ec7-136e-4c5e-a85d-053353cd6fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[1.]\n",
      " [2.]]\n",
      "(3, 1)\n",
      "dJ_dw = [[0.99845601 2.39507239]]\n",
      "dJ_db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1., 2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]).T, np.array([[1,0,1]]).T\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dJ_dw = \" + str(grads[\"dJ_dw\"]))\n",
    "print (\"dJ_db = \" + str(grads[\"dJ_db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a208bd4f-00c9-4926-a3ce-0def74d30c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, C= 1, verbose = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (number of examples,1)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "   \n",
    "    costs = [] # keep history for plotting if necessary \n",
    "    \n",
    "    for i in range(num_iterations):        \n",
    "\n",
    "    \n",
    "        # YOUR_CODE.  Call to compute cost and gradient \n",
    "        # START_CODE   \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        # END_CODE \n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dJ_dw = grads[\"dJ_dw\"]\n",
    "        dJ_db = grads[\"dJ_db\"]\n",
    "        \n",
    "        # YOUR_CODE.  Update paramaters \n",
    "        # START_CODE   \n",
    "        w = w - dJ_dw * learning_rate\n",
    "        b = b - dJ_db * learning_rate\n",
    "        # END_CODE \n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if verbose and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dJ_dw\": dJ_dw,\n",
    "             \"dJ_db\": dJ_db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3337228f-ff3a-440a-9f89-450386505b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.19033591 0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042 1.41625495]]\n",
      "db = 0.21919450454067657\n"
     ]
    }
   ],
   "source": [
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, verbose = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dJ_dw\"]))\n",
    "print (\"db = \" + str(grads[\"dJ_db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3530596e-bbcc-4092-b2e0-c763e85dec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights, a numpy array of size (1,num_px * num_px * 3)\n",
    "    b - bias, a scalar\n",
    "    X - data of size (number of examples, num_px * num_px * 3)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction - a numpy array of shape (number of examples, 1) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m,n = X.shape\n",
    "    assert (w.shape==(1,n))\n",
    " \n",
    "    # YOUR_CODE.  Compute \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    # START_CODE   \n",
    "    A= 1 / (1 + np.exp(-(b + X @ w.T)))\n",
    "    # END_CODE \n",
    "\n",
    "    # YOUR_CODE.  Convert probabilities to actual predictions 0 or 1 \n",
    "    # START_CODE   \n",
    "    Y_prediction = (A > 0.5).astype(int)\n",
    "    # END_CODE \n",
    "    \n",
    "    assert(Y_prediction.shape == (m, 1))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acb3312e-6dfb-410c-b5cf-c9eeeb2da987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = \n",
      "[[1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]]).T\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]]).T\n",
    "print (\"predictions = \\n{}\".format (predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ec84c6c-e594-4373-b22b-d762b59e5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, verbose = False, C= 1):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the functions implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (number of examples,1)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    C- regularization parameter \n",
    "    \n",
    "    Returns:\n",
    "    res -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR_CODE.\n",
    "    # START_CODE   \n",
    "\n",
    "    #  initialize parameters\n",
    "    dim = X_train.shape[0]\n",
    "    w, b = initialize_with_zeros(dim)\n",
    "\n",
    "    # run gradient descent \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, verbose)\n",
    "   \n",
    "    \n",
    "    # retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_train)\n",
    "    Y_prediction_train = predict(w, b, X_test)\n",
    "    # END_CODE \n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy= {:.3%}\".format(np.mean(Y_prediction_train == Y_train)))\n",
    "    print(\"test accuracy= {:.3%}\".format(np.mean(Y_prediction_test == Y_test)))\n",
    "    \n",
    "    res = {'costs': costs,\n",
    "           'Y_prediction_test': Y_prediction_test, \n",
    "           'Y_prediction_train' : Y_prediction_train, \n",
    "           'w' : w, \n",
    "           'b' : b,\n",
    "           'learning_rate' : learning_rate,\n",
    "           'num_iterations': num_iterations,\n",
    "           'C':C\n",
    "          }\n",
    "     return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "645ea1c2-a077-4d2f-b718-db9cc12fb2d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 209 is different from 12288)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m            \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m            \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m            \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmodel\u001b[39m\u001b[34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, verbose, C)\u001b[39m\n\u001b[32m     24\u001b[39m w, b = initialize_with_zeros(dim)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# run gradient descent \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m parameters, grads, costs = \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# retrieve parameters w and b from dictionary \"parameters\"\u001b[39;00m\n\u001b[32m     31\u001b[39m w = params[\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36moptimize\u001b[39m\u001b[34m(w, b, X, Y, num_iterations, learning_rate, C, verbose)\u001b[39m\n\u001b[32m     20\u001b[39m costs = [] \u001b[38;5;66;03m# keep history for plotting if necessary \u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):        \n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# YOUR_CODE.  Call to compute cost and gradient \u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# START_CODE   \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     grads, cost = \u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# END_CODE \u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# Retrieve derivatives from grads\u001b[39;00m\n\u001b[32m     31\u001b[39m     dJ_dw = grads[\u001b[33m\"\u001b[39m\u001b[33mdJ_dw\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mpropagate\u001b[39m\u001b[34m(w, b, X, Y, C)\u001b[39m\n\u001b[32m     17\u001b[39m m = X.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# YOUR_CODE.  implement forward propagation \u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# START_CODE   \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m Z= b + \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#Z = np.dot(X, w.T) + b\u001b[39;00m\n\u001b[32m     23\u001b[39m A=  \u001b[32m1\u001b[39m / (\u001b[32m1\u001b[39m + np.exp(-Z))\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 209 is different from 12288)"
     ]
    }
   ],
   "source": [
    "res = model(X_train= X_train_scaled,\n",
    "            Y_train=Y_train, \n",
    "            X_test=X_test_scaled, \n",
    "            Y_test= Y_test, \n",
    "            num_iterations = 3000, \n",
    "            learning_rate = 0.01, \n",
    "            verbose = True,\n",
    "            C= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c0c58-fc66-40e4-8202-4f8fd6ae0161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
